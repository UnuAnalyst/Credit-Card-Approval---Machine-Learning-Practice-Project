{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc05611-4f58-4d58-a767-ca973e3cc331",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 6067,
    "lastExecutedAt": 1708380756409,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\n\n# Load the dataset\ncc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \ncc_apps.head()\n\n# Drop the features 11 and 13\ncc_apps = cc_apps.drop([11, 13], axis=1)\n\n# Split into train and test sets\ncc_apps_train, cc_apps_test = train_test_split(cc_apps, test_size=0.33, random_state=42)\n\n# Replace the '?'s with NaN in the train and test sets\ncc_apps_train_nans_replaced = cc_apps_train.replace(\"?\", np.NaN)\ncc_apps_test_nans_replaced = cc_apps_test.replace(\"?\", np.NaN)\n\n# Impute the missing values with mean imputation\ncc_apps_train_imputed = cc_apps_train_nans_replaced.fillna(cc_apps_train_nans_replaced.mean())\ncc_apps_test_imputed = cc_apps_test_nans_replaced.fillna(cc_apps_train_nans_replaced.mean())\n\n# Iterate over each column of cc_apps_train_imputed\nfor col in cc_apps_train_imputed.columns:\n    # Check if the column is of object type\n    if cc_apps_train_imputed[col].dtypes == \"object\":\n        # Impute with the most frequent value\n        cc_apps_train_imputed = cc_apps_train_imputed.fillna(\n            cc_apps_train_imputed[col].value_counts().index[0]\n        )\n        cc_apps_test_imputed = cc_apps_test_imputed.fillna(\n            cc_apps_train_imputed[col].value_counts().index[0]\n        )\n\n# Convert the categorical features in the train and test sets independently\ncc_apps_train_cat_encoding = pd.get_dummies(cc_apps_train_imputed)\ncc_apps_test_cat_encoding = pd.get_dummies(cc_apps_test_imputed)\n\n# Reindex the columns of the test set aligning with the train set\ncc_apps_test_cat_encoding = cc_apps_test_cat_encoding.reindex(\n    columns=cc_apps_train_cat_encoding.columns, fill_value=0\n)\n\n# Segregate features and labels into separate variables\nX_train, y_train = (\n    cc_apps_train_cat_encoding.iloc[:, :-1].values,\n    cc_apps_train_cat_encoding.iloc[:, [-1]].values,\n)\nX_test, y_test = (\n    cc_apps_test_cat_encoding.iloc[:, :-1].values,\n    cc_apps_test_cat_encoding.iloc[:, [-1]].values,\n)\n\n# Instantiate MinMaxScaler and use it to rescale X_train and X_test\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)\n\n# Instantiate a LogisticRegression classifier with default parameter values\nlogreg = LogisticRegression()\n\n# Fit logreg to the train set\nlogreg.fit(rescaledX_train, y_train)\n\n# Use logreg to predict instances from the test set and store it\ny_pred = logreg.predict(rescaledX_test)\n\n# Print the confusion matrix of the logreg model\nprint(confusion_matrix(y_test, y_pred))\n\n# Define the grid of values for tol and max_iter\ntol = [0.01, 0.001, 0.0001]\nmax_iter = [100, 150, 200]\n\n# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\nparam_grid = dict(tol=tol, max_iter=max_iter)\n\n# Instantiate GridSearchCV with the required parameters\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n\n# Fit grid_model to the data\ngrid_model_result = grid_model.fit(rescaledX_train, y_train)\n\n# Summarize results\nbest_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_score, best_params))\n\n# Extract the best model and evaluate it on the test set\nbest_model = grid_model_result.best_estimator_\nprint(\n    \"Accuracy of logistic regression classifier: \",\n    best_model.score(rescaledX_test, y_test),\n)\n",
    "outputsMetadata": {
     "0": {
      "height": 137,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"int\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m cc_apps_test_nans_replaced \u001b[38;5;241m=\u001b[39m cc_apps_test\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mNaN)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Impute the missing values with mean imputation\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m cc_apps_train_imputed \u001b[38;5;241m=\u001b[39m cc_apps_train_nans_replaced\u001b[38;5;241m.\u001b[39mfillna(cc_apps_train_nans_replaced\u001b[38;5;241m.\u001b[39mmean())\n\u001b[0;32m     26\u001b[0m cc_apps_test_imputed \u001b[38;5;241m=\u001b[39m cc_apps_test_nans_replaced\u001b[38;5;241m.\u001b[39mfillna(cc_apps_train_nans_replaced\u001b[38;5;241m.\u001b[39mmean())\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Iterate over each column of cc_apps_train_imputed\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Anacondaa\\Lib\\site-packages\\pandas\\core\\generic.py:11556\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.mean\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11539\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[0;32m  11540\u001b[0m     _num_doc,\n\u001b[0;32m  11541\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn the mean of the values over the requested axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11554\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11555\u001b[0m ):\n\u001b[1;32m> 11556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Anacondaa\\Lib\\site-packages\\pandas\\core\\generic.py:11201\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[1;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[0;32m  11195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  11196\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  11199\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m  11200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[1;32m> 11201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[0;32m  11202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanmean, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m  11203\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Anacondaa\\Lib\\site-packages\\pandas\\core\\generic.py:11158\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[1;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[0;32m  11154\u001b[0m     nv\u001b[38;5;241m.\u001b[39mvalidate_stat_func((), kwargs, fname\u001b[38;5;241m=\u001b[39mname)\n\u001b[0;32m  11156\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m> 11158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce(\n\u001b[0;32m  11159\u001b[0m     func, name\u001b[38;5;241m=\u001b[39mname, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[0;32m  11160\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Anacondaa\\Lib\\site-packages\\pandas\\core\\frame.py:10519\u001b[0m, in \u001b[0;36mDataFrame._reduce\u001b[1;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[0;32m  10515\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m  10517\u001b[0m \u001b[38;5;66;03m# After possibly _get_data and transposing, we are now in the\u001b[39;00m\n\u001b[0;32m  10518\u001b[0m \u001b[38;5;66;03m#  simple case where we can use BlockManager.reduce\u001b[39;00m\n\u001b[1;32m> 10519\u001b[0m res \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39mreduce(blk_func)\n\u001b[0;32m  10520\u001b[0m out \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39m_constructor(res)\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m  10521\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Anacondaa\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:1534\u001b[0m, in \u001b[0;36mBlockManager.reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m   1532\u001b[0m res_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1533\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m-> 1534\u001b[0m     nbs \u001b[38;5;241m=\u001b[39m blk\u001b[38;5;241m.\u001b[39mreduce(func)\n\u001b[0;32m   1535\u001b[0m     res_blocks\u001b[38;5;241m.\u001b[39mextend(nbs)\n\u001b[0;32m   1537\u001b[0m index \u001b[38;5;241m=\u001b[39m Index([\u001b[38;5;28;01mNone\u001b[39;00m])  \u001b[38;5;66;03m# placeholder\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Anacondaa\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:339\u001b[0m, in \u001b[0;36mBlock.reduce\u001b[1;34m(self, func)\u001b[0m\n\u001b[0;32m    333\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreduce\u001b[39m(\u001b[38;5;28mself\u001b[39m, func) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;66;03m# We will apply the function and reshape the result into a single-row\u001b[39;00m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;66;03m#  Block with the same mgr_locs; squeezing will be done at a higher level\u001b[39;00m\n\u001b[0;32m    337\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m--> 339\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m    341\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    342\u001b[0m         \u001b[38;5;66;03m# TODO(EA2D): special case not needed with 2D EAs\u001b[39;00m\n\u001b[0;32m    343\u001b[0m         res_values \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[result]])\n",
      "File \u001b[1;32m~\\anaconda3\\Anacondaa\\Lib\\site-packages\\pandas\\core\\frame.py:10482\u001b[0m, in \u001b[0;36mDataFrame._reduce.<locals>.blk_func\u001b[1;34m(values, axis)\u001b[0m\n\u001b[0;32m  10480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m values\u001b[38;5;241m.\u001b[39m_reduce(name, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m  10481\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m> 10482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Anacondaa\\Lib\\site-packages\\pandas\\core\\nanops.py:96\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     95\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 96\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;66;03m# we want to transform an object array\u001b[39;00m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;66;03m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;66;03m# object arrays that contain strings\u001b[39;00m\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(args[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[1;32m~\\anaconda3\\Anacondaa\\Lib\\site-packages\\pandas\\core\\nanops.py:158\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[1;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[0;32m    156\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 158\u001b[0m     result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Anacondaa\\Lib\\site-packages\\pandas\\core\\nanops.py:421\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[1;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    419\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[1;32m--> 421\u001b[0m result \u001b[38;5;241m=\u001b[39m func(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, mask\u001b[38;5;241m=\u001b[39mmask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[0;32m    424\u001b[0m     result \u001b[38;5;241m=\u001b[39m _wrap_results(result, orig_values\u001b[38;5;241m.\u001b[39mdtype, fill_value\u001b[38;5;241m=\u001b[39miNaT)\n",
      "File \u001b[1;32m~\\anaconda3\\Anacondaa\\Lib\\site-packages\\pandas\\core\\nanops.py:727\u001b[0m, in \u001b[0;36mnanmean\u001b[1;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[0;32m    724\u001b[0m     dtype_count \u001b[38;5;241m=\u001b[39m dtype\n\u001b[0;32m    726\u001b[0m count \u001b[38;5;241m=\u001b[39m _get_counts(values\u001b[38;5;241m.\u001b[39mshape, mask, axis, dtype\u001b[38;5;241m=\u001b[39mdtype_count)\n\u001b[1;32m--> 727\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _ensure_numeric(values\u001b[38;5;241m.\u001b[39msum(axis, dtype\u001b[38;5;241m=\u001b[39mdtype_sum))\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    730\u001b[0m     count \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, count)\n",
      "File \u001b[1;32m~\\anaconda3\\Anacondaa\\Lib\\site-packages\\numpy\\core\\_methods.py:49\u001b[0m, in \u001b[0;36m_sum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sum\u001b[39m(a, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, keepdims\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     48\u001b[0m          initial\u001b[38;5;241m=\u001b[39m_NoValue, where\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m umr_sum(a, axis, dtype, out, keepdims, initial, where)\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate str (not \"int\") to str"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Load the dataset\n",
    "cc_apps = pd.read_csv(\"cc_approvals.data\", header=None) \n",
    "cc_apps.head()\n",
    "\n",
    "# Drop the features 11 and 13\n",
    "cc_apps = cc_apps.drop([11, 13], axis=1)\n",
    "\n",
    "# Split into train and test sets\n",
    "cc_apps_train, cc_apps_test = train_test_split(cc_apps, test_size=0.33, random_state=42)\n",
    "\n",
    "# Replace the '?'s with NaN in the train and test sets\n",
    "cc_apps_train_nans_replaced = cc_apps_train.replace(\"?\", np.NaN)\n",
    "cc_apps_test_nans_replaced = cc_apps_test.replace(\"?\", np.NaN)\n",
    "\n",
    "# Impute the missing values with mean imputation\n",
    "cc_apps_train_imputed = cc_apps_train_nans_replaced.fillna(cc_apps_train_nans_replaced.mean())\n",
    "cc_apps_test_imputed = cc_apps_test_nans_replaced.fillna(cc_apps_train_nans_replaced.mean())\n",
    "\n",
    "# Iterate over each column of cc_apps_train_imputed\n",
    "for col in cc_apps_train_imputed.columns:\n",
    "    # Check if the column is of object type\n",
    "    if cc_apps_train_imputed[col].dtypes == \"object\":\n",
    "        # Impute with the most frequent value\n",
    "        cc_apps_train_imputed = cc_apps_train_imputed.fillna(\n",
    "            cc_apps_train_imputed[col].value_counts().index[0]\n",
    "        )\n",
    "        cc_apps_test_imputed = cc_apps_test_imputed.fillna(\n",
    "            cc_apps_train_imputed[col].value_counts().index[0]\n",
    "        )\n",
    "\n",
    "# Convert the categorical features in the train and test sets independently\n",
    "cc_apps_train_cat_encoding = pd.get_dummies(cc_apps_train_imputed)\n",
    "cc_apps_test_cat_encoding = pd.get_dummies(cc_apps_test_imputed)\n",
    "\n",
    "# Reindex the columns of the test set aligning with the train set\n",
    "cc_apps_test_cat_encoding = cc_apps_test_cat_encoding.reindex(\n",
    "    columns=cc_apps_train_cat_encoding.columns, fill_value=0\n",
    ")\n",
    "\n",
    "# Segregate features and labels into separate variables\n",
    "X_train, y_train = (\n",
    "    cc_apps_train_cat_encoding.iloc[:, :-1].values,\n",
    "    cc_apps_train_cat_encoding.iloc[:, [-1]].values,\n",
    ")\n",
    "X_test, y_test = (\n",
    "    cc_apps_test_cat_encoding.iloc[:, :-1].values,\n",
    "    cc_apps_test_cat_encoding.iloc[:, [-1]].values,\n",
    ")\n",
    "\n",
    "# Instantiate MinMaxScaler and use it to rescale X_train and X_test\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "rescaledX_train = scaler.fit_transform(X_train)\n",
    "rescaledX_test = scaler.transform(X_test)\n",
    "\n",
    "# Instantiate a LogisticRegression classifier with default parameter values\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# Fit logreg to the train set\n",
    "logreg.fit(rescaledX_train, y_train)\n",
    "\n",
    "# Use logreg to predict instances from the test set and store it\n",
    "y_pred = logreg.predict(rescaledX_test)\n",
    "\n",
    "# Print the confusion matrix of the logreg model\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Define the grid of values for tol and max_iter\n",
    "tol = [0.01, 0.001, 0.0001]\n",
    "max_iter = [100, 150, 200]\n",
    "\n",
    "# Create a dictionary where tol and max_iter are keys and the lists of their values are the corresponding values\n",
    "param_grid = dict(tol=tol, max_iter=max_iter)\n",
    "\n",
    "# Instantiate GridSearchCV with the required parameters\n",
    "grid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n",
    "\n",
    "# Fit grid_model to the data\n",
    "grid_model_result = grid_model.fit(rescaledX_train, y_train)\n",
    "\n",
    "# Summarize results\n",
    "best_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\n",
    "print(\"Best: %f using %s\" % (best_score, best_params))\n",
    "\n",
    "# Extract the best model and evaluate it on the test set\n",
    "best_model = grid_model_result.best_estimator_\n",
    "print(\n",
    "    \"Accuracy of logistic regression classifier: \",\n",
    "    best_model.score(rescaledX_test, y_test),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4925b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
